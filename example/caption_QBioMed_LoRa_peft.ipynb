{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE5GJ6s7y0Xo"
      },
      "source": [
        "## Fine-tune BioMedGPT models using 🤗 `LoRa` adapters, `transformers` & `bitsandbytes`\n",
        "\n",
        "In this tutorial we will cover how we can fine-tune large language models using the very recent `peft` library and `bitsandbytes` for loading large models in 8-bit.\n",
        "The fine-tuning method will rely on a recent method called \"Low Rank Adapters\" (LoRA), instead of fine-tuning the entire model you just have to fine-tune these adapters and load them properly inside the model.\n",
        "After fine-tuning the model you can also share your adapters on the 🤗 Hub and load them very easily. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfBzP8gWzkpv"
      },
      "source": [
        "### Install requirements\n",
        "\n",
        "First, run the cells below to install the requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otj46qRbtpnd",
        "outputId": "5b35610b-081a-47c7-a96f-f87265f5e66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting git+https://github.com/Baijiong-Lin/LoRA-Torch\n",
            "  Cloning https://github.com/Baijiong-Lin/LoRA-Torch to /tmp/pip-req-build-o6z7afxi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Baijiong-Lin/LoRA-Torch /tmp/pip-req-build-o6z7afxi\n",
            "  Resolved https://github.com/Baijiong-Lin/LoRA-Torch to commit 4bfed6820b64fcf47064c30f30606a190a4f0d2e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: loratorch\n",
            "  Building wheel for loratorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for loratorch: filename=loratorch-0.1.0-py3-none-any.whl size=8035 sha256=1f7730c42797768c2e0d0e48efd2d82760e63a79121325e032263d66c189199c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7yn926ns/wheels/ac/c2/91/382d592efd0b164c3ea41b378f8650adf774bfc25925bcf5d8\n",
            "Successfully built loratorch\n",
            "Installing collected packages: loratorch\n",
            "Successfully installed loratorch-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install git+https://github.com/Baijiong-Lin/LoRA-Torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone --single-branch --branch feature/add_transformers https://github.com/OFA-Sys/OFA.git\n",
        "!pip install OFA/transformers/\n",
        "!git clone https://huggingface.co/OFA-Sys/OFA-tiny\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEblvNRnDLUK",
        "outputId": "ab5ffc9f-b489-43e2-a427-bb995fd1c2a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'OFA'...\n",
            "remote: Enumerating objects: 5745, done.\u001b[K\n",
            "remote: Counting objects: 100% (916/916), done.\u001b[K\n",
            "remote: Compressing objects: 100% (254/254), done.\u001b[K\n",
            "remote: Total 5745 (delta 695), reused 662 (delta 662), pack-reused 4829\u001b[K\n",
            "Receiving objects: 100% (5745/5745), 97.78 MiB | 23.96 MiB/s, done.\n",
            "Resolving deltas: 100% (2243/2243), done.\n",
            "Processing ./OFA/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.18.0.dev0)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1 (from transformers==4.18.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0.dev0) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.3.2)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3916748 sha256=5d474633ba507ee19ce185d9426132872626a99619fffd336adce69ad0a86ab5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3kz1zb6s/wheels/0b/bc/ea/00b6b8998c20c4fe55affe6062a2cddda80308ef9bd5d5877c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895240 sha256=fae20444fc774e86c83f42bff465ee49fcccccc9241ef9ef4e4ba16555bc3755\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.13.3 transformers-4.18.0.dev0\n",
            "Cloning into 'OFA-tiny'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Total 55 (delta 0), reused 0 (delta 0), pack-reused 55\u001b[K\n",
            "Unpacking objects: 100% (55/55), 541.39 KiB | 1.90 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "!pip install sentencepiece ## For faster tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umhG1v5QDLRQ",
        "outputId": "73d54cba-f168-4c1a-b39d-c4b7829ec5ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOtwYRI3zzXI"
      },
      "source": [
        "### Model loading\n",
        "\n",
        "Here let's load the `OFA-Tiny Version` model, its weights in half-precision (float16) are about 13GB on the Hub! If we load them in 8-bit we would require around 7GB of memory instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "cg3fiQOvmI3Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "from torchvision import transforms\n",
        "from transformers import OFATokenizer, OFAModel\n",
        "from transformers.models.ofa.generate import sequence_generator\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "from torchsummary import summary\n",
        "from transformers import DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import EncoderDecoderConfig\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import EncoderDecoderModel\n",
        "from transformers import CONFIG_MAPPING\n",
        "\n",
        "import loralib as lora\n",
        "import loratorch as LoraT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTSZntA1iUG"
      },
      "source": [
        "### Post-processing on the model\n",
        "\n",
        "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in `float32` for stability. We also cast the output of the last layer in `float32` for the same reasons."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_dir='OFA-tiny'\n",
        "tokenizer = OFATokenizer.from_pretrained(ckpt_dir)\n",
        "model = OFAModel.from_pretrained(ckpt_dir, use_cache=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDgv_cXXHvsz",
        "outputId": "4a6e5488-b3aa-4738-b778-fb0d0118a188"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OFA-tiny\n",
            "<super: <class 'OFATokenizer'>, <OFATokenizer object>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "T-gy-LxM0yAi"
      },
      "outputs": [],
      "source": [
        "# for param in model.parameters():\n",
        "#   param.requires_grad = False  # freeze the model - train adapters later\n",
        "#   if param.ndim == 1:\n",
        "#     # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "#     param.data = param.data.to(torch.float32)\n",
        "\n",
        "# model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "\n",
        "# class CastOutputToFloat(nn.Sequential):\n",
        "\n",
        "#   def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "# model.encoder , model.decoder= CastOutputToFloat(model.encoder) ,  CastOutputToFloat(model.decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwOTr7B3NlM3"
      },
      "source": [
        "### Apply LoRA\n",
        "\n",
        "Here comes the magic with `LOra`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_Lora_model` utility function from `Loratorch`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_r = 4\n",
        "USE_LORA = True\n",
        "\n",
        "def make_lora_layer(layer, lora_r=4):\n",
        "    new_layer = LoraT.Linear(\n",
        "        in_features=layer.in_features,\n",
        "        out_features=layer.out_features,\n",
        "        bias=layer.bias is not None,  # Fixing the bias check\n",
        "        r=lora_r\n",
        "    )\n",
        "\n",
        "    new_layer.weight = nn.Parameter(layer.weight.detach().clone())  # Cloning the tensor\n",
        "\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias = nn.Parameter(layer.bias.detach().clone())  # Cloning the tensor\n",
        "\n",
        "    return new_layer\n",
        "\n",
        "def make_lora_replace(model, depth=1, path=\"\", verbose=True):\n",
        "    if depth > 10:\n",
        "        return model\n",
        "\n",
        "    if isinstance(model, nn.Linear) and (\"self_attn\" in path or \"cross_attn\" in path):\n",
        "        if verbose:\n",
        "            print(f\"Find linear {path}:\", type(model))\n",
        "        return make_lora_layer(model)\n",
        "\n",
        "    for key, module in model.named_children():  # Using named_children() for cleaner iteration\n",
        "        if isinstance(module, nn.Linear) and (\"self_attn\" in path or \"cross_attn\" in path):\n",
        "            layer = make_lora_layer(module)\n",
        "            setattr(model, key, layer)\n",
        "            if verbose:\n",
        "                print(f\"Find linear {path}:{key} :\", type(module))\n",
        "\n",
        "        elif isinstance(module, nn.ModuleList):\n",
        "            for i, elem in enumerate(module):\n",
        "                layer = make_lora_replace(elem, depth+1, f\"{path}:{key}[{i}]\", verbose=verbose)\n",
        "                if layer is not None:\n",
        "                    module[i] = layer\n",
        "\n",
        "        elif isinstance(module, nn.ModuleDict):\n",
        "            for module_key, item in module.items():\n",
        "                layer = make_lora_replace(item, depth+1, f\"{path}:{key}:{module_key}\", verbose=verbose)\n",
        "                if layer is not None:\n",
        "                    module[module_key] = layer\n",
        "\n",
        "        else:\n",
        "            layer = make_lora_replace(module, depth+1, f\"{path}:{key}\", verbose=verbose)\n",
        "            if layer is not None:\n",
        "                setattr(model, key, layer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8bwoex_SXxQ4"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iwHGzKBN6wk",
        "outputId": "54284499-46b8-4a6b-cf8c-c54a5c1a85c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Find linear :encoder:layers[0]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[0]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[0]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[0]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[1]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[1]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[1]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[1]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[2]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[2]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[2]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[2]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[3]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[3]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[3]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :encoder:layers[3]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:cross_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:cross_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:cross_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[0]:cross_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:cross_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:cross_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:cross_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[1]:cross_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:cross_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:cross_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:cross_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[2]:cross_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:self_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:self_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:self_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:self_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:cross_attn:k_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:cross_attn:v_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:cross_attn:q_proj : <class 'torch.nn.modules.linear.Linear'>\n",
            "Find linear :decoder:layers[3]:cross_attn:out_proj : <class 'torch.nn.modules.linear.Linear'>\n"
          ]
        }
      ],
      "source": [
        "if USE_LORA:\n",
        "    make_lora_replace(model, verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_r = 4 ## Try different max matrix ranks for different results\n",
        "\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before LoRA: {total_trainable_params}\")\n",
        "\n",
        "## Apply LoRA\n",
        "LoraT.mark_only_lora_as_trainable(model)\n",
        "\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after LoRA: {total_trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_rAAL9jI0pZ",
        "outputId": "439c471d-b461-4811-f6c4-6422754fbc3c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters before LoRA: 33586640\n",
            "Total trainable parameters after LoRA: 98304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"deberta\" not in name:\n",
        "        print(name)\n",
        "#             print(param.shape)\n",
        "        param.requires_grad = True\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6N24rnvL1-V",
        "outputId": "ecfc71bf-8264-4952-fae1-242ce6c1d3bf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.layernorm_embedding.weight\n",
            "encoder.layernorm_embedding.bias\n",
            "encoder.embed_tokens.weight\n",
            "encoder.type_embedding.weight\n",
            "encoder.embed_images.conv1.weight\n",
            "encoder.embed_images.bn1.weight\n",
            "encoder.embed_images.bn1.bias\n",
            "encoder.embed_images.layer1.0.conv1.weight\n",
            "encoder.embed_images.layer1.0.bn1.weight\n",
            "encoder.embed_images.layer1.0.bn1.bias\n",
            "encoder.embed_images.layer1.0.conv2.weight\n",
            "encoder.embed_images.layer1.0.bn2.weight\n",
            "encoder.embed_images.layer1.0.bn2.bias\n",
            "encoder.embed_images.layer1.0.conv3.weight\n",
            "encoder.embed_images.layer1.0.bn3.weight\n",
            "encoder.embed_images.layer1.0.bn3.bias\n",
            "encoder.embed_images.layer1.0.downsample.0.weight\n",
            "encoder.embed_images.layer1.0.downsample.1.weight\n",
            "encoder.embed_images.layer1.0.downsample.1.bias\n",
            "encoder.embed_images.layer1.1.conv1.weight\n",
            "encoder.embed_images.layer1.1.bn1.weight\n",
            "encoder.embed_images.layer1.1.bn1.bias\n",
            "encoder.embed_images.layer1.1.conv2.weight\n",
            "encoder.embed_images.layer1.1.bn2.weight\n",
            "encoder.embed_images.layer1.1.bn2.bias\n",
            "encoder.embed_images.layer1.1.conv3.weight\n",
            "encoder.embed_images.layer1.1.bn3.weight\n",
            "encoder.embed_images.layer1.1.bn3.bias\n",
            "encoder.embed_images.layer1.2.conv1.weight\n",
            "encoder.embed_images.layer1.2.bn1.weight\n",
            "encoder.embed_images.layer1.2.bn1.bias\n",
            "encoder.embed_images.layer1.2.conv2.weight\n",
            "encoder.embed_images.layer1.2.bn2.weight\n",
            "encoder.embed_images.layer1.2.bn2.bias\n",
            "encoder.embed_images.layer1.2.conv3.weight\n",
            "encoder.embed_images.layer1.2.bn3.weight\n",
            "encoder.embed_images.layer1.2.bn3.bias\n",
            "encoder.embed_images.layer2.0.conv1.weight\n",
            "encoder.embed_images.layer2.0.bn1.weight\n",
            "encoder.embed_images.layer2.0.bn1.bias\n",
            "encoder.embed_images.layer2.0.conv2.weight\n",
            "encoder.embed_images.layer2.0.bn2.weight\n",
            "encoder.embed_images.layer2.0.bn2.bias\n",
            "encoder.embed_images.layer2.0.conv3.weight\n",
            "encoder.embed_images.layer2.0.bn3.weight\n",
            "encoder.embed_images.layer2.0.bn3.bias\n",
            "encoder.embed_images.layer2.0.downsample.0.weight\n",
            "encoder.embed_images.layer2.0.downsample.1.weight\n",
            "encoder.embed_images.layer2.0.downsample.1.bias\n",
            "encoder.embed_images.layer2.1.conv1.weight\n",
            "encoder.embed_images.layer2.1.bn1.weight\n",
            "encoder.embed_images.layer2.1.bn1.bias\n",
            "encoder.embed_images.layer2.1.conv2.weight\n",
            "encoder.embed_images.layer2.1.bn2.weight\n",
            "encoder.embed_images.layer2.1.bn2.bias\n",
            "encoder.embed_images.layer2.1.conv3.weight\n",
            "encoder.embed_images.layer2.1.bn3.weight\n",
            "encoder.embed_images.layer2.1.bn3.bias\n",
            "encoder.embed_images.layer2.2.conv1.weight\n",
            "encoder.embed_images.layer2.2.bn1.weight\n",
            "encoder.embed_images.layer2.2.bn1.bias\n",
            "encoder.embed_images.layer2.2.conv2.weight\n",
            "encoder.embed_images.layer2.2.bn2.weight\n",
            "encoder.embed_images.layer2.2.bn2.bias\n",
            "encoder.embed_images.layer2.2.conv3.weight\n",
            "encoder.embed_images.layer2.2.bn3.weight\n",
            "encoder.embed_images.layer2.2.bn3.bias\n",
            "encoder.embed_images.layer2.3.conv1.weight\n",
            "encoder.embed_images.layer2.3.bn1.weight\n",
            "encoder.embed_images.layer2.3.bn1.bias\n",
            "encoder.embed_images.layer2.3.conv2.weight\n",
            "encoder.embed_images.layer2.3.bn2.weight\n",
            "encoder.embed_images.layer2.3.bn2.bias\n",
            "encoder.embed_images.layer2.3.conv3.weight\n",
            "encoder.embed_images.layer2.3.bn3.weight\n",
            "encoder.embed_images.layer2.3.bn3.bias\n",
            "encoder.embed_images.layer3.0.conv1.weight\n",
            "encoder.embed_images.layer3.0.bn1.weight\n",
            "encoder.embed_images.layer3.0.bn1.bias\n",
            "encoder.embed_images.layer3.0.conv2.weight\n",
            "encoder.embed_images.layer3.0.bn2.weight\n",
            "encoder.embed_images.layer3.0.bn2.bias\n",
            "encoder.embed_images.layer3.0.conv3.weight\n",
            "encoder.embed_images.layer3.0.bn3.weight\n",
            "encoder.embed_images.layer3.0.bn3.bias\n",
            "encoder.embed_images.layer3.0.downsample.0.weight\n",
            "encoder.embed_images.layer3.0.downsample.1.weight\n",
            "encoder.embed_images.layer3.0.downsample.1.bias\n",
            "encoder.embed_images.layer3.1.conv1.weight\n",
            "encoder.embed_images.layer3.1.bn1.weight\n",
            "encoder.embed_images.layer3.1.bn1.bias\n",
            "encoder.embed_images.layer3.1.conv2.weight\n",
            "encoder.embed_images.layer3.1.bn2.weight\n",
            "encoder.embed_images.layer3.1.bn2.bias\n",
            "encoder.embed_images.layer3.1.conv3.weight\n",
            "encoder.embed_images.layer3.1.bn3.weight\n",
            "encoder.embed_images.layer3.1.bn3.bias\n",
            "encoder.embed_images.layer3.2.conv1.weight\n",
            "encoder.embed_images.layer3.2.bn1.weight\n",
            "encoder.embed_images.layer3.2.bn1.bias\n",
            "encoder.embed_images.layer3.2.conv2.weight\n",
            "encoder.embed_images.layer3.2.bn2.weight\n",
            "encoder.embed_images.layer3.2.bn2.bias\n",
            "encoder.embed_images.layer3.2.conv3.weight\n",
            "encoder.embed_images.layer3.2.bn3.weight\n",
            "encoder.embed_images.layer3.2.bn3.bias\n",
            "encoder.embed_images.layer3.3.conv1.weight\n",
            "encoder.embed_images.layer3.3.bn1.weight\n",
            "encoder.embed_images.layer3.3.bn1.bias\n",
            "encoder.embed_images.layer3.3.conv2.weight\n",
            "encoder.embed_images.layer3.3.bn2.weight\n",
            "encoder.embed_images.layer3.3.bn2.bias\n",
            "encoder.embed_images.layer3.3.conv3.weight\n",
            "encoder.embed_images.layer3.3.bn3.weight\n",
            "encoder.embed_images.layer3.3.bn3.bias\n",
            "encoder.embed_images.layer3.4.conv1.weight\n",
            "encoder.embed_images.layer3.4.bn1.weight\n",
            "encoder.embed_images.layer3.4.bn1.bias\n",
            "encoder.embed_images.layer3.4.conv2.weight\n",
            "encoder.embed_images.layer3.4.bn2.weight\n",
            "encoder.embed_images.layer3.4.bn2.bias\n",
            "encoder.embed_images.layer3.4.conv3.weight\n",
            "encoder.embed_images.layer3.4.bn3.weight\n",
            "encoder.embed_images.layer3.4.bn3.bias\n",
            "encoder.embed_images.layer3.5.conv1.weight\n",
            "encoder.embed_images.layer3.5.bn1.weight\n",
            "encoder.embed_images.layer3.5.bn1.bias\n",
            "encoder.embed_images.layer3.5.conv2.weight\n",
            "encoder.embed_images.layer3.5.bn2.weight\n",
            "encoder.embed_images.layer3.5.bn2.bias\n",
            "encoder.embed_images.layer3.5.conv3.weight\n",
            "encoder.embed_images.layer3.5.bn3.weight\n",
            "encoder.embed_images.layer3.5.bn3.bias\n",
            "encoder.image_proj.weight\n",
            "encoder.image_proj.bias\n",
            "encoder.patch_layernorm_embedding.weight\n",
            "encoder.patch_layernorm_embedding.bias\n",
            "encoder.embed_positions.weight\n",
            "encoder.embed_image_positions.weight\n",
            "encoder.pos_ln.weight\n",
            "encoder.pos_ln.bias\n",
            "encoder.image_pos_ln.weight\n",
            "encoder.image_pos_ln.bias\n",
            "encoder.pos_q_linear.weight\n",
            "encoder.pos_q_linear.bias\n",
            "encoder.pos_k_linear.weight\n",
            "encoder.pos_k_linear.bias\n",
            "encoder.layers.0.self_attn.c_attn\n",
            "encoder.layers.0.self_attn.k_proj.weight\n",
            "encoder.layers.0.self_attn.k_proj.bias\n",
            "encoder.layers.0.self_attn.k_proj.w_lora_A\n",
            "encoder.layers.0.self_attn.k_proj.w_lora_B\n",
            "encoder.layers.0.self_attn.v_proj.weight\n",
            "encoder.layers.0.self_attn.v_proj.bias\n",
            "encoder.layers.0.self_attn.v_proj.w_lora_A\n",
            "encoder.layers.0.self_attn.v_proj.w_lora_B\n",
            "encoder.layers.0.self_attn.q_proj.weight\n",
            "encoder.layers.0.self_attn.q_proj.bias\n",
            "encoder.layers.0.self_attn.q_proj.w_lora_A\n",
            "encoder.layers.0.self_attn.q_proj.w_lora_B\n",
            "encoder.layers.0.self_attn.out_proj.weight\n",
            "encoder.layers.0.self_attn.out_proj.bias\n",
            "encoder.layers.0.self_attn.out_proj.w_lora_A\n",
            "encoder.layers.0.self_attn.out_proj.w_lora_B\n",
            "encoder.layers.0.self_attn_layer_norm.weight\n",
            "encoder.layers.0.self_attn_layer_norm.bias\n",
            "encoder.layers.0.self_attn_mid_layer_norm.weight\n",
            "encoder.layers.0.self_attn_mid_layer_norm.bias\n",
            "encoder.layers.0.fc1.weight\n",
            "encoder.layers.0.fc1.bias\n",
            "encoder.layers.0.fc2.weight\n",
            "encoder.layers.0.fc2.bias\n",
            "encoder.layers.0.ffn_layer_norm.weight\n",
            "encoder.layers.0.ffn_layer_norm.bias\n",
            "encoder.layers.0.final_layer_norm.weight\n",
            "encoder.layers.0.final_layer_norm.bias\n",
            "encoder.layers.1.self_attn.c_attn\n",
            "encoder.layers.1.self_attn.k_proj.weight\n",
            "encoder.layers.1.self_attn.k_proj.bias\n",
            "encoder.layers.1.self_attn.k_proj.w_lora_A\n",
            "encoder.layers.1.self_attn.k_proj.w_lora_B\n",
            "encoder.layers.1.self_attn.v_proj.weight\n",
            "encoder.layers.1.self_attn.v_proj.bias\n",
            "encoder.layers.1.self_attn.v_proj.w_lora_A\n",
            "encoder.layers.1.self_attn.v_proj.w_lora_B\n",
            "encoder.layers.1.self_attn.q_proj.weight\n",
            "encoder.layers.1.self_attn.q_proj.bias\n",
            "encoder.layers.1.self_attn.q_proj.w_lora_A\n",
            "encoder.layers.1.self_attn.q_proj.w_lora_B\n",
            "encoder.layers.1.self_attn.out_proj.weight\n",
            "encoder.layers.1.self_attn.out_proj.bias\n",
            "encoder.layers.1.self_attn.out_proj.w_lora_A\n",
            "encoder.layers.1.self_attn.out_proj.w_lora_B\n",
            "encoder.layers.1.self_attn_layer_norm.weight\n",
            "encoder.layers.1.self_attn_layer_norm.bias\n",
            "encoder.layers.1.self_attn_mid_layer_norm.weight\n",
            "encoder.layers.1.self_attn_mid_layer_norm.bias\n",
            "encoder.layers.1.fc1.weight\n",
            "encoder.layers.1.fc1.bias\n",
            "encoder.layers.1.fc2.weight\n",
            "encoder.layers.1.fc2.bias\n",
            "encoder.layers.1.ffn_layer_norm.weight\n",
            "encoder.layers.1.ffn_layer_norm.bias\n",
            "encoder.layers.1.final_layer_norm.weight\n",
            "encoder.layers.1.final_layer_norm.bias\n",
            "encoder.layers.2.self_attn.c_attn\n",
            "encoder.layers.2.self_attn.k_proj.weight\n",
            "encoder.layers.2.self_attn.k_proj.bias\n",
            "encoder.layers.2.self_attn.k_proj.w_lora_A\n",
            "encoder.layers.2.self_attn.k_proj.w_lora_B\n",
            "encoder.layers.2.self_attn.v_proj.weight\n",
            "encoder.layers.2.self_attn.v_proj.bias\n",
            "encoder.layers.2.self_attn.v_proj.w_lora_A\n",
            "encoder.layers.2.self_attn.v_proj.w_lora_B\n",
            "encoder.layers.2.self_attn.q_proj.weight\n",
            "encoder.layers.2.self_attn.q_proj.bias\n",
            "encoder.layers.2.self_attn.q_proj.w_lora_A\n",
            "encoder.layers.2.self_attn.q_proj.w_lora_B\n",
            "encoder.layers.2.self_attn.out_proj.weight\n",
            "encoder.layers.2.self_attn.out_proj.bias\n",
            "encoder.layers.2.self_attn.out_proj.w_lora_A\n",
            "encoder.layers.2.self_attn.out_proj.w_lora_B\n",
            "encoder.layers.2.self_attn_layer_norm.weight\n",
            "encoder.layers.2.self_attn_layer_norm.bias\n",
            "encoder.layers.2.self_attn_mid_layer_norm.weight\n",
            "encoder.layers.2.self_attn_mid_layer_norm.bias\n",
            "encoder.layers.2.fc1.weight\n",
            "encoder.layers.2.fc1.bias\n",
            "encoder.layers.2.fc2.weight\n",
            "encoder.layers.2.fc2.bias\n",
            "encoder.layers.2.ffn_layer_norm.weight\n",
            "encoder.layers.2.ffn_layer_norm.bias\n",
            "encoder.layers.2.final_layer_norm.weight\n",
            "encoder.layers.2.final_layer_norm.bias\n",
            "encoder.layers.3.self_attn.c_attn\n",
            "encoder.layers.3.self_attn.k_proj.weight\n",
            "encoder.layers.3.self_attn.k_proj.bias\n",
            "encoder.layers.3.self_attn.k_proj.w_lora_A\n",
            "encoder.layers.3.self_attn.k_proj.w_lora_B\n",
            "encoder.layers.3.self_attn.v_proj.weight\n",
            "encoder.layers.3.self_attn.v_proj.bias\n",
            "encoder.layers.3.self_attn.v_proj.w_lora_A\n",
            "encoder.layers.3.self_attn.v_proj.w_lora_B\n",
            "encoder.layers.3.self_attn.q_proj.weight\n",
            "encoder.layers.3.self_attn.q_proj.bias\n",
            "encoder.layers.3.self_attn.q_proj.w_lora_A\n",
            "encoder.layers.3.self_attn.q_proj.w_lora_B\n",
            "encoder.layers.3.self_attn.out_proj.weight\n",
            "encoder.layers.3.self_attn.out_proj.bias\n",
            "encoder.layers.3.self_attn.out_proj.w_lora_A\n",
            "encoder.layers.3.self_attn.out_proj.w_lora_B\n",
            "encoder.layers.3.self_attn_layer_norm.weight\n",
            "encoder.layers.3.self_attn_layer_norm.bias\n",
            "encoder.layers.3.self_attn_mid_layer_norm.weight\n",
            "encoder.layers.3.self_attn_mid_layer_norm.bias\n",
            "encoder.layers.3.fc1.weight\n",
            "encoder.layers.3.fc1.bias\n",
            "encoder.layers.3.fc2.weight\n",
            "encoder.layers.3.fc2.bias\n",
            "encoder.layers.3.ffn_layer_norm.weight\n",
            "encoder.layers.3.ffn_layer_norm.bias\n",
            "encoder.layers.3.final_layer_norm.weight\n",
            "encoder.layers.3.final_layer_norm.bias\n",
            "encoder.layer_norm.weight\n",
            "encoder.layer_norm.bias\n",
            "encoder.token_rel_pos_table_list.0.weight\n",
            "encoder.token_rel_pos_table_list.1.weight\n",
            "encoder.token_rel_pos_table_list.2.weight\n",
            "encoder.token_rel_pos_table_list.3.weight\n",
            "encoder.image_rel_pos_table_list.0.weight\n",
            "encoder.image_rel_pos_table_list.1.weight\n",
            "encoder.image_rel_pos_table_list.2.weight\n",
            "encoder.image_rel_pos_table_list.3.weight\n",
            "decoder.layers.0.self_attn.c_attn\n",
            "decoder.layers.0.self_attn.k_proj.weight\n",
            "decoder.layers.0.self_attn.k_proj.bias\n",
            "decoder.layers.0.self_attn.k_proj.w_lora_A\n",
            "decoder.layers.0.self_attn.k_proj.w_lora_B\n",
            "decoder.layers.0.self_attn.v_proj.weight\n",
            "decoder.layers.0.self_attn.v_proj.bias\n",
            "decoder.layers.0.self_attn.v_proj.w_lora_A\n",
            "decoder.layers.0.self_attn.v_proj.w_lora_B\n",
            "decoder.layers.0.self_attn.q_proj.weight\n",
            "decoder.layers.0.self_attn.q_proj.bias\n",
            "decoder.layers.0.self_attn.q_proj.w_lora_A\n",
            "decoder.layers.0.self_attn.q_proj.w_lora_B\n",
            "decoder.layers.0.self_attn.out_proj.weight\n",
            "decoder.layers.0.self_attn.out_proj.bias\n",
            "decoder.layers.0.self_attn.out_proj.w_lora_A\n",
            "decoder.layers.0.self_attn.out_proj.w_lora_B\n",
            "decoder.layers.0.self_attn_layer_norm.weight\n",
            "decoder.layers.0.self_attn_layer_norm.bias\n",
            "decoder.layers.0.self_attn_mid_layer_norm.weight\n",
            "decoder.layers.0.self_attn_mid_layer_norm.bias\n",
            "decoder.layers.0.cross_attn.c_attn\n",
            "decoder.layers.0.cross_attn.k_proj.weight\n",
            "decoder.layers.0.cross_attn.k_proj.bias\n",
            "decoder.layers.0.cross_attn.k_proj.w_lora_A\n",
            "decoder.layers.0.cross_attn.k_proj.w_lora_B\n",
            "decoder.layers.0.cross_attn.v_proj.weight\n",
            "decoder.layers.0.cross_attn.v_proj.bias\n",
            "decoder.layers.0.cross_attn.v_proj.w_lora_A\n",
            "decoder.layers.0.cross_attn.v_proj.w_lora_B\n",
            "decoder.layers.0.cross_attn.q_proj.weight\n",
            "decoder.layers.0.cross_attn.q_proj.bias\n",
            "decoder.layers.0.cross_attn.q_proj.w_lora_A\n",
            "decoder.layers.0.cross_attn.q_proj.w_lora_B\n",
            "decoder.layers.0.cross_attn.out_proj.weight\n",
            "decoder.layers.0.cross_attn.out_proj.bias\n",
            "decoder.layers.0.cross_attn.out_proj.w_lora_A\n",
            "decoder.layers.0.cross_attn.out_proj.w_lora_B\n",
            "decoder.layers.0.cross_attn_layer_norm.weight\n",
            "decoder.layers.0.cross_attn_layer_norm.bias\n",
            "decoder.layers.0.cross_attn_mid_layer_norm.weight\n",
            "decoder.layers.0.cross_attn_mid_layer_norm.bias\n",
            "decoder.layers.0.fc1.weight\n",
            "decoder.layers.0.fc1.bias\n",
            "decoder.layers.0.fc2.weight\n",
            "decoder.layers.0.fc2.bias\n",
            "decoder.layers.0.ffn_layer_norm.weight\n",
            "decoder.layers.0.ffn_layer_norm.bias\n",
            "decoder.layers.0.final_layer_norm.weight\n",
            "decoder.layers.0.final_layer_norm.bias\n",
            "decoder.layers.1.self_attn.c_attn\n",
            "decoder.layers.1.self_attn.k_proj.weight\n",
            "decoder.layers.1.self_attn.k_proj.bias\n",
            "decoder.layers.1.self_attn.k_proj.w_lora_A\n",
            "decoder.layers.1.self_attn.k_proj.w_lora_B\n",
            "decoder.layers.1.self_attn.v_proj.weight\n",
            "decoder.layers.1.self_attn.v_proj.bias\n",
            "decoder.layers.1.self_attn.v_proj.w_lora_A\n",
            "decoder.layers.1.self_attn.v_proj.w_lora_B\n",
            "decoder.layers.1.self_attn.q_proj.weight\n",
            "decoder.layers.1.self_attn.q_proj.bias\n",
            "decoder.layers.1.self_attn.q_proj.w_lora_A\n",
            "decoder.layers.1.self_attn.q_proj.w_lora_B\n",
            "decoder.layers.1.self_attn.out_proj.weight\n",
            "decoder.layers.1.self_attn.out_proj.bias\n",
            "decoder.layers.1.self_attn.out_proj.w_lora_A\n",
            "decoder.layers.1.self_attn.out_proj.w_lora_B\n",
            "decoder.layers.1.self_attn_layer_norm.weight\n",
            "decoder.layers.1.self_attn_layer_norm.bias\n",
            "decoder.layers.1.self_attn_mid_layer_norm.weight\n",
            "decoder.layers.1.self_attn_mid_layer_norm.bias\n",
            "decoder.layers.1.cross_attn.c_attn\n",
            "decoder.layers.1.cross_attn.k_proj.weight\n",
            "decoder.layers.1.cross_attn.k_proj.bias\n",
            "decoder.layers.1.cross_attn.k_proj.w_lora_A\n",
            "decoder.layers.1.cross_attn.k_proj.w_lora_B\n",
            "decoder.layers.1.cross_attn.v_proj.weight\n",
            "decoder.layers.1.cross_attn.v_proj.bias\n",
            "decoder.layers.1.cross_attn.v_proj.w_lora_A\n",
            "decoder.layers.1.cross_attn.v_proj.w_lora_B\n",
            "decoder.layers.1.cross_attn.q_proj.weight\n",
            "decoder.layers.1.cross_attn.q_proj.bias\n",
            "decoder.layers.1.cross_attn.q_proj.w_lora_A\n",
            "decoder.layers.1.cross_attn.q_proj.w_lora_B\n",
            "decoder.layers.1.cross_attn.out_proj.weight\n",
            "decoder.layers.1.cross_attn.out_proj.bias\n",
            "decoder.layers.1.cross_attn.out_proj.w_lora_A\n",
            "decoder.layers.1.cross_attn.out_proj.w_lora_B\n",
            "decoder.layers.1.cross_attn_layer_norm.weight\n",
            "decoder.layers.1.cross_attn_layer_norm.bias\n",
            "decoder.layers.1.cross_attn_mid_layer_norm.weight\n",
            "decoder.layers.1.cross_attn_mid_layer_norm.bias\n",
            "decoder.layers.1.fc1.weight\n",
            "decoder.layers.1.fc1.bias\n",
            "decoder.layers.1.fc2.weight\n",
            "decoder.layers.1.fc2.bias\n",
            "decoder.layers.1.ffn_layer_norm.weight\n",
            "decoder.layers.1.ffn_layer_norm.bias\n",
            "decoder.layers.1.final_layer_norm.weight\n",
            "decoder.layers.1.final_layer_norm.bias\n",
            "decoder.layers.2.self_attn.c_attn\n",
            "decoder.layers.2.self_attn.k_proj.weight\n",
            "decoder.layers.2.self_attn.k_proj.bias\n",
            "decoder.layers.2.self_attn.k_proj.w_lora_A\n",
            "decoder.layers.2.self_attn.k_proj.w_lora_B\n",
            "decoder.layers.2.self_attn.v_proj.weight\n",
            "decoder.layers.2.self_attn.v_proj.bias\n",
            "decoder.layers.2.self_attn.v_proj.w_lora_A\n",
            "decoder.layers.2.self_attn.v_proj.w_lora_B\n",
            "decoder.layers.2.self_attn.q_proj.weight\n",
            "decoder.layers.2.self_attn.q_proj.bias\n",
            "decoder.layers.2.self_attn.q_proj.w_lora_A\n",
            "decoder.layers.2.self_attn.q_proj.w_lora_B\n",
            "decoder.layers.2.self_attn.out_proj.weight\n",
            "decoder.layers.2.self_attn.out_proj.bias\n",
            "decoder.layers.2.self_attn.out_proj.w_lora_A\n",
            "decoder.layers.2.self_attn.out_proj.w_lora_B\n",
            "decoder.layers.2.self_attn_layer_norm.weight\n",
            "decoder.layers.2.self_attn_layer_norm.bias\n",
            "decoder.layers.2.self_attn_mid_layer_norm.weight\n",
            "decoder.layers.2.self_attn_mid_layer_norm.bias\n",
            "decoder.layers.2.cross_attn.c_attn\n",
            "decoder.layers.2.cross_attn.k_proj.weight\n",
            "decoder.layers.2.cross_attn.k_proj.bias\n",
            "decoder.layers.2.cross_attn.k_proj.w_lora_A\n",
            "decoder.layers.2.cross_attn.k_proj.w_lora_B\n",
            "decoder.layers.2.cross_attn.v_proj.weight\n",
            "decoder.layers.2.cross_attn.v_proj.bias\n",
            "decoder.layers.2.cross_attn.v_proj.w_lora_A\n",
            "decoder.layers.2.cross_attn.v_proj.w_lora_B\n",
            "decoder.layers.2.cross_attn.q_proj.weight\n",
            "decoder.layers.2.cross_attn.q_proj.bias\n",
            "decoder.layers.2.cross_attn.q_proj.w_lora_A\n",
            "decoder.layers.2.cross_attn.q_proj.w_lora_B\n",
            "decoder.layers.2.cross_attn.out_proj.weight\n",
            "decoder.layers.2.cross_attn.out_proj.bias\n",
            "decoder.layers.2.cross_attn.out_proj.w_lora_A\n",
            "decoder.layers.2.cross_attn.out_proj.w_lora_B\n",
            "decoder.layers.2.cross_attn_layer_norm.weight\n",
            "decoder.layers.2.cross_attn_layer_norm.bias\n",
            "decoder.layers.2.cross_attn_mid_layer_norm.weight\n",
            "decoder.layers.2.cross_attn_mid_layer_norm.bias\n",
            "decoder.layers.2.fc1.weight\n",
            "decoder.layers.2.fc1.bias\n",
            "decoder.layers.2.fc2.weight\n",
            "decoder.layers.2.fc2.bias\n",
            "decoder.layers.2.ffn_layer_norm.weight\n",
            "decoder.layers.2.ffn_layer_norm.bias\n",
            "decoder.layers.2.final_layer_norm.weight\n",
            "decoder.layers.2.final_layer_norm.bias\n",
            "decoder.layers.3.self_attn.c_attn\n",
            "decoder.layers.3.self_attn.k_proj.weight\n",
            "decoder.layers.3.self_attn.k_proj.bias\n",
            "decoder.layers.3.self_attn.k_proj.w_lora_A\n",
            "decoder.layers.3.self_attn.k_proj.w_lora_B\n",
            "decoder.layers.3.self_attn.v_proj.weight\n",
            "decoder.layers.3.self_attn.v_proj.bias\n",
            "decoder.layers.3.self_attn.v_proj.w_lora_A\n",
            "decoder.layers.3.self_attn.v_proj.w_lora_B\n",
            "decoder.layers.3.self_attn.q_proj.weight\n",
            "decoder.layers.3.self_attn.q_proj.bias\n",
            "decoder.layers.3.self_attn.q_proj.w_lora_A\n",
            "decoder.layers.3.self_attn.q_proj.w_lora_B\n",
            "decoder.layers.3.self_attn.out_proj.weight\n",
            "decoder.layers.3.self_attn.out_proj.bias\n",
            "decoder.layers.3.self_attn.out_proj.w_lora_A\n",
            "decoder.layers.3.self_attn.out_proj.w_lora_B\n",
            "decoder.layers.3.self_attn_layer_norm.weight\n",
            "decoder.layers.3.self_attn_layer_norm.bias\n",
            "decoder.layers.3.self_attn_mid_layer_norm.weight\n",
            "decoder.layers.3.self_attn_mid_layer_norm.bias\n",
            "decoder.layers.3.cross_attn.c_attn\n",
            "decoder.layers.3.cross_attn.k_proj.weight\n",
            "decoder.layers.3.cross_attn.k_proj.bias\n",
            "decoder.layers.3.cross_attn.k_proj.w_lora_A\n",
            "decoder.layers.3.cross_attn.k_proj.w_lora_B\n",
            "decoder.layers.3.cross_attn.v_proj.weight\n",
            "decoder.layers.3.cross_attn.v_proj.bias\n",
            "decoder.layers.3.cross_attn.v_proj.w_lora_A\n",
            "decoder.layers.3.cross_attn.v_proj.w_lora_B\n",
            "decoder.layers.3.cross_attn.q_proj.weight\n",
            "decoder.layers.3.cross_attn.q_proj.bias\n",
            "decoder.layers.3.cross_attn.q_proj.w_lora_A\n",
            "decoder.layers.3.cross_attn.q_proj.w_lora_B\n",
            "decoder.layers.3.cross_attn.out_proj.weight\n",
            "decoder.layers.3.cross_attn.out_proj.bias\n",
            "decoder.layers.3.cross_attn.out_proj.w_lora_A\n",
            "decoder.layers.3.cross_attn.out_proj.w_lora_B\n",
            "decoder.layers.3.cross_attn_layer_norm.weight\n",
            "decoder.layers.3.cross_attn_layer_norm.bias\n",
            "decoder.layers.3.cross_attn_mid_layer_norm.weight\n",
            "decoder.layers.3.cross_attn_mid_layer_norm.bias\n",
            "decoder.layers.3.fc1.weight\n",
            "decoder.layers.3.fc1.bias\n",
            "decoder.layers.3.fc2.weight\n",
            "decoder.layers.3.fc2.bias\n",
            "decoder.layers.3.ffn_layer_norm.weight\n",
            "decoder.layers.3.ffn_layer_norm.bias\n",
            "decoder.layers.3.final_layer_norm.weight\n",
            "decoder.layers.3.final_layer_norm.bias\n",
            "decoder.layernorm_embedding.weight\n",
            "decoder.layernorm_embedding.bias\n",
            "decoder.embed_positions.weight\n",
            "decoder.embed_image_positions.weight\n",
            "decoder.pos_ln.weight\n",
            "decoder.pos_ln.bias\n",
            "decoder.image_pos_ln.weight\n",
            "decoder.image_pos_ln.bias\n",
            "decoder.self_pos_q_linear.weight\n",
            "decoder.self_pos_q_linear.bias\n",
            "decoder.self_pos_k_linear.weight\n",
            "decoder.self_pos_k_linear.bias\n",
            "decoder.cross_pos_q_linear.weight\n",
            "decoder.cross_pos_q_linear.bias\n",
            "decoder.cross_pos_k_linear.weight\n",
            "decoder.cross_pos_k_linear.bias\n",
            "decoder.code_layernorm_embedding.weight\n",
            "decoder.code_layernorm_embedding.bias\n",
            "decoder.layer_norm.weight\n",
            "decoder.layer_norm.bias\n",
            "decoder.token_rel_pos_table_list.0.weight\n",
            "decoder.token_rel_pos_table_list.1.weight\n",
            "decoder.token_rel_pos_table_list.2.weight\n",
            "decoder.token_rel_pos_table_list.3.weight\n",
            "decoder.image_rel_pos_table_list.0.weight\n",
            "decoder.image_rel_pos_table_list.1.weight\n",
            "decoder.image_rel_pos_table_list.2.weight\n",
            "decoder.image_rel_pos_table_list.3.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdjWif4CVXR6"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxB6UV5XAvvP"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"ybelkada/opt-6.7b-lora\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S65GcxNGA9kz"
      },
      "source": [
        "## Load adapters from the Hub\n",
        "\n",
        "You can also directly load adapters from the Hub using the commands below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHYljmTjj5wX"
      },
      "source": [
        "## Inference\n",
        "\n",
        "You can then directly use the trained model or the model that you have loaded from the 🤗 Hub for inference as you would do it usually in `transformers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZL8ZbcJCHoy"
      },
      "source": [
        "As you can see by fine-tuning for few steps we have almost recovered the quote from Albert Einstein that is present in the [training data](https://huggingface.co/datasets/Abirate/english_quotes)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}